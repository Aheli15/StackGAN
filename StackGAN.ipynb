{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "StackGAN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmMHjPTvVg0K",
        "colab_type": "code",
        "outputId": "56d057a3-f1c2-4ecc-a337-05f7c5de08f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d97ixxBedTY4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "186666f7-619a-45b1-9fb3-0e3622f1a4d8"
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow is already loaded. Please restart the runtime to change versions.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8w3vAJ81Jo51",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVDmLZEraE-W",
        "colab_type": "code",
        "outputId": "f043338f-310a-48d5-8be5-f5e9a607d6ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "os.listdir('/content/gdrive/My Drive/CUB Dataset/CUB_200_2011/')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['parts',\n",
              " 'images.txt',\n",
              " 'train_test_split.txt',\n",
              " 'image_class_labels.txt',\n",
              " 'classes.txt',\n",
              " 'bounding_boxes.txt',\n",
              " 'attributes',\n",
              " 'images',\n",
              " 'README.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJr5Txuku-aA",
        "colab_type": "text"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVpK69iAJMUi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras import Input, Model\n",
        "from tensorflow.keras.layers import LeakyReLU, BatchNormalization, ReLU, Activation\n",
        "from tensorflow.keras.layers import UpSampling2D, Conv2D, Concatenate, Dense, concatenate\n",
        "from tensorflow.keras.layers import Flatten, Lambda, Reshape, ZeroPadding2D, add\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2zKSLVFOzBT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_ca_model():\n",
        "    input_layer = Input(shape=(1024,))\n",
        "    x = Dense(256)(input_layer)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    model = Model(inputs=[input_layer], outputs=[x])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4at1AFuoO7j3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_embedding_compressor_model():\n",
        "    input_layer = Input(shape=(1024,))\n",
        "    x = Dense(128)(input_layer)\n",
        "    x = ReLU()(x)\n",
        "    model = Model(inputs=[input_layer], outputs=[x])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXHF2nzNPBI-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_c(x):\n",
        "\tmean = x[:, :128]\n",
        "\tlog_sigma = x[:, 128:]\n",
        "\tstddev = tf.math.exp(log_sigma)\n",
        "\tepsilon = K.random_normal(shape=K.constant((mean.shape[1], ), dtype='int32'))\n",
        "\tc = mean + stddev * epsilon\n",
        "\treturn c"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Qse6nduPU5U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ConvBlock(x, num_kernels, kernel_size=(4,4), strides=2, activation=True):\n",
        "\tx = Conv2D(num_kernels, kernel_size=kernel_size, padding='same', strides=strides, use_bias=False, kernel_initializer='he_uniform')(x)\n",
        "\tx = BatchNormalization(gamma_initializer='ones', beta_initializer='zeros')(x)\n",
        "\tif activation:\n",
        "\t\tx = LeakyReLU(alpha=0.2)(x)\n",
        "\treturn x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgdifwIVPWBm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def UpSamplingBlock(x, num_kernels):\n",
        "\tx = UpSampling2D(size=(2,2))(x)\n",
        "\tx = Conv2D(num_kernels, kernel_size=(3,3), padding='same', strides=1, use_bias=False, kernel_initializer='he_uniform')(x)\n",
        "\tx = BatchNormalization(gamma_initializer='ones', beta_initializer='zeros')(x)\n",
        "\tx = ReLU()(x)\n",
        "\treturn x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-ZORZvWxHIh",
        "colab_type": "text"
      },
      "source": [
        "### Stage 1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbxxU6j0wuxq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_stage1_generator():\n",
        "\n",
        "    input_layer = Input(shape=(1024,))    \n",
        "    ca = Dense(256)(input_layer)\n",
        "    ca = LeakyReLU(alpha=0.2)(ca)\n",
        "\n",
        "\t# Obtain the conditioned text\n",
        "    c = Lambda(generate_c)(ca)\n",
        "\n",
        "    input_layer2 = Input(shape=(100,))\n",
        "\n",
        "    concat = Concatenate(axis=1)([c, input_layer2])\n",
        "\n",
        "    x = Dense(16384, use_bias=False)(concat)\n",
        "    x = ReLU()(x)\n",
        "    x = Reshape((4, 4, 1024), input_shape=(16384,))(x)\n",
        "\n",
        "    x = UpSamplingBlock(x, 512)\n",
        "    x = UpSamplingBlock(x, 256)\n",
        "    x = UpSamplingBlock(x, 128)\n",
        "    x = UpSamplingBlock(x, 64)\n",
        "\n",
        "    x = Conv2D(3, kernel_size=3, padding='same', strides=1, use_bias=False, kernel_initializer='he_uniform')(x)\n",
        "    x = Activation('tanh')(x)\n",
        "\n",
        "    model = Model(inputs=[input_layer1, input_layer2], outputs=[x, ca])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gr88hrrAuWDz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_stage1_discriminator():\n",
        "\tinput_layer1 = Input(shape=(64, 64, 3))\n",
        "\n",
        "\tx = Conv2D(64, kernel_size=(4,4), strides=2, padding='same', use_bias=False, kernel_initializer='he_uniform')(input_layer1)\n",
        "\tx = LeakyReLU(alpha=0.2)(x)\n",
        "\n",
        "\tx = ConvBlock(x, 128)\n",
        "\tx = ConvBlock(x, 256)\n",
        "\tx = ConvBlock(x, 512)\n",
        "\n",
        "\t# Obtain the compressed and spatially replicated text embedding\n",
        "\tinput_layer2 = Input(shape=(4, 4, 128))\n",
        "\tconcat = concatenate([x, input_layer2])\n",
        "\n",
        "\tx1 = Conv2D(512, kernel_size=(1,1), padding='same', strides=1, use_bias=False, kernel_initializer='he_uniform')(concat)\n",
        "\tx1 = BatchNormalization(gamma_initializer='ones', beta_initializer='zeros')(x)\n",
        "\tx1 = LeakyReLU(alpha=0.2)(x)\n",
        "\n",
        "\t# Flatten and add a FC layer to predict.\n",
        "\tx1 = Flatten()(x1)\n",
        "\tx1 = Dense(1)(x1)\n",
        "\tx1 = Activation('sigmoid')(x1)\n",
        "\n",
        "\tmodel = Model(inputs=[input_layer1, input_layer2], outputs=[x1])\n",
        "\treturn model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQ4YNapBxDIC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_stage1_adversarial_model(gen_model, dis_model):\n",
        "    input_layer = Input(shape=(1024,))\n",
        "    input_layer2 = Input(shape=(100,))\n",
        "    input_layer3 = Input(shape=(4, 4, 128))\n",
        "\n",
        "    x, mean_logsigma = gen_model([input_layer, input_layer2])\n",
        "\n",
        "    dis_model.trainable = False\n",
        "    valid = dis_model([x, input_layer3])\n",
        "\n",
        "    model = Model(inputs=[input_layer, input_layer2, input_layer3], outputs=[valid, mean_logsigma])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mg-bjotBxV3b",
        "colab_type": "text"
      },
      "source": [
        "### Stage 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9OWkA9KPrOV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def residual_block(input):\n",
        "\tx = Conv2D(512, kernel_size=(3,3), padding='same', use_bias=False,\n",
        "\t\t\t\tkernel_initializer='he_uniform')(input)\n",
        "\tx = BatchNormalization(gamma_initializer='ones', beta_initializer='zeros')(x)\n",
        "\tx = ReLU()(x)\n",
        "\t\n",
        "\tx = Conv2D(512, kernel_size=(3,3), padding='same', use_bias=False,\n",
        "\t\t\t\tkernel_initializer='he_uniform')(x)\n",
        "\tx = BatchNormalization(gamma_initializer='ones', beta_initializer='zeros')(x)\n",
        "\t\n",
        "\tx = add([x, input])\n",
        "\tx = ReLU()(x)\n",
        "\n",
        "\treturn x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_djjnCCQBRc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def joint_block(inputs):\n",
        "    c = inputs[0]\n",
        "    x = inputs[1]\n",
        "\n",
        "    c = K.expand_dims(c, axis=1)\n",
        "    c = K.expand_dims(c, axis=1)\n",
        "    c = K.tile(c, [1, 16, 16, 1])\n",
        "    return K.concatenate([c, x], axis=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWlBEcKVQFF6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_stage2_generator():\n",
        "    # 1. CA Augmentation Network\n",
        "    input_layer = Input(shape=(1024,))\n",
        "    input_lr_images = Input(shape=(64, 64, 3))\n",
        "\n",
        "    ca = Dense(256)(input_layer)\n",
        "    mean_logsigma = LeakyReLU(alpha=0.2)(ca)\n",
        "    c = Lambda(generate_c)(mean_logsigma)\n",
        "\n",
        "    # 2. Image Encoder\n",
        "    x = ZeroPadding2D(padding=(1, 1))(input_lr_images)\n",
        "    x = Conv2D(128, kernel_size=(3, 3), strides=1, use_bias=False)(x)\n",
        "    x = ReLU()(x)\n",
        "\n",
        "    x = ZeroPadding2D(padding=(1, 1))(x)\n",
        "    x = Conv2D(256, kernel_size=(4, 4), strides=2, use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "\n",
        "    x = ZeroPadding2D(padding=(1, 1))(x)\n",
        "    x = Conv2D(512, kernel_size=(4, 4), strides=2, use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "\n",
        "    # 3. Joint\n",
        "    c_code = Lambda(joint_block)([c, x])\n",
        "\n",
        "    x = ZeroPadding2D(padding=(1, 1))(c_code)\n",
        "    x = Conv2D(512, kernel_size=(3, 3), strides=1, use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "\n",
        "    # 4. Residual blocks\n",
        "    x = residual_block(x)\n",
        "    x = residual_block(x)\n",
        "    x = residual_block(x)\n",
        "    x = residual_block(x)\n",
        "\n",
        "    # 5. Upsampling blocks\n",
        "    x = UpSamplingBlock(x, 512)\n",
        "    x = UpSamplingBlock(x, 256)\n",
        "    x = UpSamplingBlock(x, 128)\n",
        "    x = UpSamplingBlock(x, 64)\n",
        "\n",
        "    x = Conv2D(3, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n",
        "    x = Activation('tanh')(x)\n",
        "\n",
        "    model = Model(inputs=[input_layer, input_lr_images], outputs=[x, mean_logsigma])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVieNwBtQ80y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_stage2_discriminator():\n",
        "\tinput_layer1 = Input(shape=(256, 256, 3))\n",
        "\n",
        "\tx = Conv2D(64, kernel_size=(4,4), padding='same', strides=2, use_bias=False,\n",
        "\t\t\t\tkernel_initializer='he_uniform')(input_layer1)\n",
        "\tx = LeakyReLU(alpha=0.2)(x)\n",
        "\n",
        "\tx = ConvBlock(x, 128)\n",
        "\tx = ConvBlock(x, 256)\n",
        "\tx = ConvBlock(x, 512)\n",
        "\tx = ConvBlock(x, 1024)\n",
        "\tx = ConvBlock(x, 2048)\n",
        "\tx = ConvBlock(x, 1024, (1,1), 1)\n",
        "\tx = ConvBlock(x, 512, (1,1), 1, False)\n",
        "\n",
        "\tx1 = ConvBlock(x, 128, (1,1), 1)\n",
        "\tx1 = ConvBlock(x1, 128, (3,3), 1)\n",
        "\tx1 = ConvBlock(x1, 512, (3,3), 1, False)\n",
        "\n",
        "\tx2 = add([x, x1])\n",
        "\tx2 = LeakyReLU(alpha=0.2)(x2)\n",
        "\n",
        "\t# Concatenate compressed and spatially replicated embedding\n",
        "\tinput_layer2 = Input(shape=(4, 4, 128))\n",
        "\tconcat = concatenate([x2, input_layer2])\n",
        "\n",
        "\tx3 = Conv2D(512, kernel_size=(1,1), strides=1, padding='same', kernel_initializer='he_uniform')(concat)\n",
        "\tx3 = BatchNormalization(gamma_initializer='ones', beta_initializer='zeros')(x3)\n",
        "\tx3 = LeakyReLU(alpha=0.2)(x3)\n",
        "\n",
        "\t# Flatten and add a FC layer\n",
        "\tx3 = Flatten()(x3)\n",
        "\tx3 = Dense(1)(x3)\n",
        "\tx3 = Activation('sigmoid')(x3)\n",
        "\n",
        "\tmodel = Model(inputs=[input_layer1, input_layer2], outputs=[x3])\n",
        "\treturn model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nc-N6YVwSxe_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_stage2_adversarial_model(gen_model2, dis_model, gen_model1):\n",
        "    \n",
        "    embeddings_input_layer = Input(shape=(1024, ))\n",
        "    noise_input_layer = Input(shape=(100, ))\n",
        "    compressed_embedding_input_layer = Input(shape=(4, 4, 128))\n",
        "\n",
        "    gen_model1.trainable = False\n",
        "    dis_model.trainable = False\n",
        "\n",
        "    lr_images, mean_logsigma1 = gen_model1([embeddings_input_layer, noise_input_layer])\n",
        "    hr_images, mean_logsigma2 = gen_model2([embeddings_input_layer, lr_images])\n",
        "    valid = dis_model([hr_images, compressed_embedding_input_layer])\n",
        "\n",
        "    model = Model(inputs=[embeddings_input_layer, noise_input_layer, compressed_embedding_input_layer], outputs=[valid, mean_logsigma2])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mqjpq-Kq_c7k",
        "colab_type": "text"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlbRbUcyIEgA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "condition_dim = 128\n",
        "z_dim = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDdaQLQbABCA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def KL_loss(y_true, y_pred):\n",
        "    mean = y_pred[:, :128]\n",
        "    logsigma = y_pred[:, :128]\n",
        "    loss = -logsigma + .5 * (-1 + K.exp(2. * logsigma) + K.square(mean))\n",
        "    loss = K.mean(loss)\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7CeITeoACfd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def custom_generator_loss(y_true, y_pred):\n",
        "    # Calculate binary cross entropy loss\n",
        "    return K.binary_crossentropy(y_true, y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kwSWi51AuMo",
        "colab_type": "text"
      },
      "source": [
        "### Stage 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IFzM5IzAf74",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image_size = 64\n",
        "batch_size_stage1 = 64\n",
        "stage1_generator_lr = 0.0002\n",
        "stage1_discriminator_lr = 0.0002\n",
        "stage1_lr_decay_step = 600\n",
        "epochs_stage1 = 1000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgMo6PLYTM62",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_stage1(X_train, y_train, embeddings_train, X_test, y_test, embeddings_test):\n",
        "\n",
        "    dis_optimizer = Adam(lr=stage1_discriminator_lr, beta_1=0.5, beta_2=0.999)\n",
        "    gen_optimizer = Adam(lr=stage1_generator_lr, beta_1=0.5, beta_2=0.999)\n",
        "\n",
        "    ca_model = build_ca_model()\n",
        "    ca_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\n",
        "\n",
        "    stage1_dis = build_stage1_discriminator()\n",
        "    stage1_dis.compile(loss='binary_crossentropy', optimizer=dis_optimizer)\n",
        "\n",
        "    stage1_gen = build_stage1_generator()\n",
        "    stage1_gen.compile(loss=\"mse\", optimizer=gen_optimizer)\n",
        "\n",
        "    embedding_compressor_model = build_embedding_compressor_model()\n",
        "    embedding_compressor_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\n",
        "\n",
        "    adversarial_model = build_stage1_adversarial_model(gen_model=stage1_gen, dis_model=stage1_dis)\n",
        "    adversarial_model.compile(loss=['binary_crossentropy', KL_loss], loss_weights=[1, 2.0], optimizer=gen_optimizer, metrics=None)\n",
        "\n",
        "    real_labels = np.ones((batch_size_stage1, 1), dtype=float) * 0.9\n",
        "    fake_labels = np.zeros((batch_size_stage1, 1), dtype=float) * 0.1\n",
        "\n",
        "    for epoch in range(epochs_stage1):\n",
        "\n",
        "        gen_losses = []\n",
        "        dis_losses = []\n",
        "        number_of_batches = int(X_train.shape[0] / batch_size_stage1)\n",
        "        for index in range(number_of_batches):\n",
        "\n",
        "            z_noise = np.random.normal(0, 1, size=(batch_size_stage1, z_dim))\n",
        "            image_batch = X_train[index * batch_size_stage1:(index + 1) * batch_size_stage1]\n",
        "            embedding_batch = embeddings_train[index * batch_size_stage1:(index + 1) * batch_size_stage1]\n",
        "            image_batch = (image_batch - 127.5) / 127.5\n",
        "\n",
        "            fake_images, _ = stage1_gen.predict([embedding_batch, z_noise], verbose=3)\n",
        "\n",
        "            compressed_embedding = embedding_compressor_model.predict_on_batch(embedding_batch)\n",
        "            compressed_embedding = np.reshape(compressed_embedding, (-1, 1, 1, condition_dim))\n",
        "            compressed_embedding = np.tile(compressed_embedding, (1, 4, 4, 1))\n",
        "\n",
        "            dis_loss_real = stage1_dis.train_on_batch([image_batch, compressed_embedding], np.reshape(real_labels, (batch_size_stage1, 1)))\n",
        "            dis_loss_fake = stage1_dis.train_on_batch([fake_images, compressed_embedding], np.reshape(fake_labels, (batch_size_stage1, 1)))\n",
        "            dis_loss_wrong = stage1_dis.train_on_batch([image_batch[:(batch_size_stage1 - 1)], compressed_embedding[1:]], np.reshape(fake_labels[1:], (batch_size_stage1-1, 1)))\n",
        "\n",
        "            d_loss = 0.5 * np.add(dis_loss_real, 0.5 * np.add(dis_loss_wrong, dis_loss_fake))\n",
        "\n",
        "            g_loss = adversarial_model.train_on_batch([embedding_batch, z_noise, compressed_embedding],[K.ones((batch_size_stage1, 1)) * 0.9, K.ones((batch_size_stage1, 256)) * 0.9])\n",
        "\n",
        "            dis_losses.append(d_loss)\n",
        "            gen_losses.append(g_loss)\n",
        "\n",
        "    stage1_gen.save_weights(\"stage1_gen.h5\")\n",
        "    stage1_dis.save_weights(\"stage1_dis.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNfaZik3G8kv",
        "colab_type": "text"
      },
      "source": [
        "### Stage 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxfasGfiDHyD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hr_image_size = (256, 256)\n",
        "lr_image_size = (64, 64)\n",
        "batch_size_stage2 = 4\n",
        "stage2_generator_lr = 0.0002\n",
        "stage2_discriminator_lr = 0.0002\n",
        "stage2_lr_decay_step = 600\n",
        "epochs_stage2 = 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvcSlEoQIPQR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_stage2(X_hr_train, y_hr_train, embeddings_train, X_hr_test, y_hr_test, embeddings_test, X_lr_train, y_lr_train, X_lr_test, y_lr_test):\n",
        "\n",
        "    dis_optimizer = Adam(lr=stage2_discriminator_lr, beta_1=0.5, beta_2=0.999)\n",
        "    gen_optimizer = Adam(lr=stage2_generator_lr, beta_1=0.5, beta_2=0.999)\n",
        "\n",
        "    stage2_dis = build_stage2_discriminator()\n",
        "    stage2_dis.compile(loss='binary_crossentropy', optimizer=dis_optimizer)\n",
        "\n",
        "    stage1_gen = build_stage1_generator()\n",
        "    stage1_gen.compile(loss=\"binary_crossentropy\", optimizer=gen_optimizer)\n",
        "\n",
        "    stage1_gen.load_weights(\"stage1_gen.h5\")\n",
        "\n",
        "    stage2_gen = build_stage2_generator()\n",
        "    stage2_gen.compile(loss=\"binary_crossentropy\", optimizer=gen_optimizer)\n",
        "\n",
        "    embedding_compressor_model = build_embedding_compressor_model()\n",
        "    embedding_compressor_model.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "\n",
        "    adversarial_model = build_stage2_adversarial_model(stage2_gen, stage2_dis, stage1_gen)\n",
        "    adversarial_model.compile(loss=['binary_crossentropy', KL_loss], loss_weights=[1.0, 2.0], optimizer=gen_optimizer, metrics=None)\n",
        "\n",
        "    real_labels = np.ones((batch_size_stage2, 1), dtype=float) * 0.9\n",
        "    fake_labels = np.zeros((batch_size_stage2, 1), dtype=float) * 0.1\n",
        "\n",
        "    for epoch in range(epochs_stage2):\n",
        "\n",
        "        gen_losses = []\n",
        "        dis_losses = []\n",
        "\n",
        "        number_of_batches = int(X_hr_train.shape[0] / batch_size_stage2)\n",
        "        for index in range(number_of_batches):\n",
        "\n",
        "            z_noise = np.random.normal(0, 1, size=(batch_size_stage2, z_dim))\n",
        "            X_hr_train_batch = X_hr_train[index * batch_size_stage2:(index + 1) * batch_size_stage2]\n",
        "            embedding_batch = embeddings_train[index * batch_size_stage2:(index + 1) * batch_size_stage2]\n",
        "            X_hr_train_batch = (X_hr_train_batch - 127.5) / 127.5\n",
        "\n",
        "            lr_fake_images, _ = stage1_gen.predict([embedding_batch, z_noise], verbose=3)\n",
        "            hr_fake_images, _ = stage2_gen.predict([embedding_batch, lr_fake_images], verbose=3)\n",
        "\n",
        "            compressed_embedding = embedding_compressor_model.predict_on_batch(embedding_batch)\n",
        "            compressed_embedding = np.reshape(compressed_embedding, (-1, 1, 1, condition_dim))\n",
        "            compressed_embedding = np.tile(compressed_embedding, (1, 4, 4, 1))\n",
        "\n",
        "            dis_loss_real = stage2_dis.train_on_batch([X_hr_train_batch, compressed_embedding], np.reshape(real_labels, (batch_size_stage2, 1)))\n",
        "            dis_loss_fake = stage2_dis.train_on_batch([hr_fake_images, compressed_embedding], np.reshape(fake_labels, (batch_size_stage2, 1)))\n",
        "            dis_loss_wrong = stage2_dis.train_on_batch([X_hr_train_batch[:(batch_size_stage2 - 1)], compressed_embedding[1:]], np.reshape(fake_labels[1:], (batch_size_stage2-1, 1)))\n",
        "            d_loss = 0.5 * np.add(dis_loss_real, 0.5 * np.add(dis_loss_wrong,  dis_loss_fake))\n",
        "\n",
        "            g_loss = adversarial_model.train_on_batch([embedding_batch, z_noise, compressed_embedding], [K.ones((batch_size_stage2, 1)) * 0.9, K.ones((batch_size_stage2, 256)) * 0.9])\n",
        "\n",
        "            dis_losses.append(d_loss)\n",
        "            gen_losses.append(g_loss)\n",
        "\n",
        "    stage2_gen.save_weights(\"stage2_gen.h5\")\n",
        "    stage2_dis.save_weights(\"stage2_dis.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjNfGzJUOrCz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}